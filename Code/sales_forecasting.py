# -*- coding: utf-8 -*-
"""Sales Forecasting.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KiV-rOeifiVF7vvc8cHNN3j46mrYzeVx

# **1. Importing the libraries & packages**
"""

!pip install catboost

import numpy as np
import pandas as pd
import os
import datetime
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split,cross_val_score,RandomizedSearchCV
from sklearn.linear_model import LinearRegression,Lasso,Ridge
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor,ExtraTreesRegressor
from catboost import CatBoostRegressor
from lightgbm import LGBMRegressor
from xgboost import XGBRegressor
from sklearn.metrics import r2_score
from scipy.stats import uniform, randint
import pickle
import warnings
warnings.filterwarnings('ignore')

df = pd.read_csv('/content/Train.csv')

"""# **2. Exploratory Data Analysis**"""

df.apply(lambda x : len(x.unique()))

df.duplicated().sum()

df.isnull().sum()

df.info()

"""# **3. Data Cleaning**"""

cat_col = []
for x in df.dtypes.index:
    if df.dtypes[x] == 'object':
        cat_col.append(x)
display(cat_col)

cat_col.remove('Item_Identifier')
cat_col.remove('Outlet_Identifier')

display(cat_col)

for col in cat_col:
    print(col,len(df[col].unique()))

for col in cat_col:
    print(col)
    print(df[col].value_counts(),'\n')
    print('-'*55)

miss_bool = df['Item_Weight'].isnull()
Item_Weight_Null = df[df['Item_Weight'].isnull()]
display(Item_Weight_Null)

Item_Weight_Null['Item_Identifier'].value_counts()

Item_Weight_Mean = df.pivot_table(values = 'Item_Weight', index = 'Item_Identifier')
display(Item_Weight_Mean)

for i, item in enumerate(df['Item_Identifier']):
    if miss_bool[i]:
        if item in Item_Weight_Mean:
            df['Item_Weight'][i] = Item_Weight_Mean.loc[item]['Item_Weight']
        else:
            df['Item_Weight'][i] = np.mean(df['Item_Weight'])

df['Item_Weight'].isna().sum()

df['Outlet_Size'].value_counts()

df['Outlet_Size'].isnull().sum()

Outlet_Size_Null = df[df['Outlet_Size'].isna()]
display(Outlet_Size_Null)

Outlet_Size_Null['Outlet_Type'].value_counts()

df.groupby(['Outlet_Type','Outlet_Size']).agg({'Outlet_Type':[np.size]})

Outlet_Size_Mode = df.pivot_table(values = 'Outlet_Size', columns = 'Outlet_Type', aggfunc = (lambda x : x.mode()[0]))
display(Outlet_Size_Mode)

miss_bool = df['Outlet_Size'].isna()
df.loc[miss_bool,'Outlet_Size'] = df.loc[miss_bool,'Outlet_Type'].apply(lambda x : Outlet_Size_Mode[x])

df['Outlet_Size'].isna().sum()

df.isna().sum()

sum(df['Item_Visibility'] == 0)

df.loc[:,'Item_Visibility'].replace([0],[df['Item_Visibility'].mean()],inplace = True)

sum(df['Item_Visibility'] == 0)

df['Item_Fat_Content'].value_counts()

"""### **After seeing the unique value counts from the `Item_Fat_Content` column, there have been some mistyping occured like the same categories were typed under different names. For further processing, all the mistypings are corrected and named under a single category. Checking out for the value counts of `Item_Fat_Content` column**"""

df['Item_Fat_Content'] = df['Item_Fat_Content'].replace({'LF' : 'Low Fat', 'low fat' : 'Low Fat', 'reg' : 'Regular'})
df['Item_Fat_Content'].value_counts()

df['New_Item_Type'] = df['Item_Identifier'].apply(lambda x : x[:2])
df['New_Item_Type'].value_counts()

df['New_Item_Type'] = df['New_Item_Type'].replace({'FD' : 'Food', 'NC' : 'Non-Consumables', 'DR' : 'Drinks'})
df['New_Item_Type'].value_counts()

df.groupby(['New_Item_Type','Item_Fat_Content']).agg({'Outlet_Type':[np.size]})

df.loc[df['New_Item_Type'] == 'Non-Consumables','Item_Fat_Content'] = 'Non-Edible'
df['Item_Fat_Content'].value_counts()

df['Outlet_Establishment_Year'].unique()

curr_time = datetime.datetime.now()
df['Outlet_Years'] = df['Outlet_Establishment_Year'].apply(lambda x: curr_time.year - x)

"""# **4. Data Visualization**

### **Plotting the Bar Graph with count of 'Item_Fat_Content'**
"""

plt.rcParams['figure.figsize'] = 15,10
plt.style.use('fivethirtyeight')
plot = sns.countplot(x = df['Item_Fat_Content'])
for p in plot.patches:
    plot.annotate(p.get_height(),(p.get_x() + p.get_width() / 2.0,p.get_height()),
                 ha = 'center',va = 'center',xytext = (0,5),textcoords = 'offset points')
plt.title('Count of Item_Fat_Content')
plt.show()

"""### **Plotting the Bar Graph with count of 'Item_Type'**"""

plot = sns.countplot(x = df['Item_Type'])
for p in plot.patches:
    plot.annotate(p.get_height(),(p.get_x() + p.get_width() / 2.0,p.get_height()),
                 ha = 'center',va = 'center',xytext = (0,5),textcoords = 'offset points')
plt.xticks(rotation = 90)
plt.title('Count of Item_Type')
plt.show()

"""### **Plotting the Bar Graph with count of 'Outlet_Establishment_Year'**"""

plot = sns.countplot(x = df['Outlet_Establishment_Year'])
for p in plot.patches:
    plot.annotate(p.get_height(),(p.get_x() + p.get_width() / 2.0,p.get_height()),
                 ha = 'center',va = 'center',xytext = (0,5),textcoords = 'offset points')
plt.title('Count of Outlet_Establishment_Year')
plt.show()

"""### **Plotting the Bar Graph with count of 'Outlet_Location_Type'**"""

plot = sns.countplot(x = df['Outlet_Location_Type'])
for p in plot.patches:
    plot.annotate(p.get_height(),(p.get_x() + p.get_width() / 2.0,p.get_height()),
                 ha = 'center',va = 'center',xytext = (0,5),textcoords = 'offset points')
plt.title('Count of Outlet_Location_Type')
plt.show()

"""### **Plotting the Bar Graph with count of 'Outlet_Size'**"""

plot = sns.countplot(x = df['Outlet_Size'])
for p in plot.patches:
    plot.annotate(p.get_height(),(p.get_x() + p.get_width() / 2.0,p.get_height()),
                 ha = 'center',va = 'center',xytext = (0,5),textcoords = 'offset points')
plt.title('Count of Outlet_Size')
plt.show()

"""### **Plotting the Bar Graph with count of 'Outlet_Type'**"""

plot = sns.countplot(x = df['Outlet_Type'])
for p in plot.patches:
    plot.annotate(p.get_height(),(p.get_x() + p.get_width() / 2.0,p.get_height()),
                 ha = 'center',va = 'center',xytext = (0,5),textcoords = 'offset points')
plt.title('Count of Outlet_Type')
plt.show()

"""### **Visualizing the data distribution of the 'Item_weight' column against the density distribution using Seaborn Distplot**"""

sns.distplot(df['Item_Weight'],bins = 20)
plt.title('Distribution of Item_Weight')
plt.show()

"""### **Correlation Heat Map**"""

numerical_features = df.select_dtypes(include=np.number).columns
numerical_df = df[numerical_features]

sns.heatmap(numerical_df.corr(), cbar=True, annot=True, square=True)
plt.title('Correlation Heat Map')
plt.show()

"""# **5. Data Preprocessing**

### **Label Encoding the `Outlet_Identifier` column and adding it as a new column `Outlet` to the dataset**
"""

le = LabelEncoder()
df['Outlet'] = le.fit_transform(df['Outlet_Identifier'])

df.dtypes

cat_col = ['Item_Fat_Content','Item_Type','Outlet_Size','Outlet_Location_Type','Outlet_Type','New_Item_Type']
for col in cat_col:
    df[col] = le.fit_transform(df[col])

"""### **One Hot Encoding the columns `Item_Fat_Content`,`Outlet_Size`,`Outlet_Location_Type`,`Outlet_Type`,`New_Item_Type` using  <span style = 'background : green'><span style = 'color : white'> get dummies </span> </span> function**"""

df = pd.get_dummies(df,columns = ['Item_Fat_Content','Outlet_Size','Outlet_Location_Type','Outlet_Type','New_Item_Type'])

x = df.drop(['Item_Identifier','Outlet_Identifier','Outlet_Establishment_Year','Item_Outlet_Sales'],axis=1)
y=df['Item_Outlet_Sales']

"""# **5. Model Fitting**"""

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.3, random_state = 10)

def train(model, x, y):
    model.fit(x, y)
    pred = model.predict(x)
    cv_score = cross_val_score(model, x, y, scoring='neg_mean_squared_error', cv=10)
    print('Model Report : \n')
    print('Scoring - neg_mean_squared_error')
    print(cv_score, '\n')
    cv_score = np.abs(np.mean(cv_score))
    print('Absolute Average of neg_mean_squared_error : ', cv_score)
    cv_score = cross_val_score(model, x, y, cv=10)
    print()
    print('R2 Score')
    print(cv_score, '\n')
    cv_score = np.mean(cv_score)
    print('Average R2 Score : ', cv_score, '\n')
    print('Accuracy for Full Data :')
    print('R2 Score : ', r2_score(y, pred), '\n')
    coef = pd.Series(model.coef_, x.columns).sort_values()
    print(coef)
    coef.plot(kind='bar', title="Model Coefficients")
    plt.show()

"""### **Fitting the Linear Regression model**"""

model = LinearRegression()
train(model,x_train,y_train)

"""

### **Fitting the Ridge model**"""

model = Ridge()
train(model, x_train, y_train)

"""

### **Fitting the Lasso model**"""

model = Lasso()
train(model, x_train, y_train)

"""### **Display of Feature Importance**"""

def train(model, x, y):
    model.fit(x, y)
    pred = model.predict(x)
    cv_score = cross_val_score(model,x,y,scoring = 'neg_mean_squared_error', cv = 10)
    print('Model Report : \n')
    print('Scoring - neg_mean_squared_error')
    print(cv_score,'\n')
    cv_score = np.abs(np.mean(cv_score))
    print('Absolute Average of neg_mean_squared_error : ',cv_score)
    cv_score = cross_val_score(model, x, y, cv = 10)
    print()
    print('R2 Score')
    print(cv_score,'\n')
    cv_score = np.mean(cv_score)
    print('Average R2 Score : ',cv_score,'\n')
    print('Accuracy for Full Data :')
    print('R2 Score : ',r2_score(y,pred),'\n')
    coef = pd.Series(model.feature_importances_, x.columns).sort_values(ascending=False)
    coef.plot(kind='bar', title="Feature Importance")
    plt.show()

"""

### **Fitting the Decision Tree Regressor model**"""

model = DecisionTreeRegressor()
train(model, x_train, y_train)

"""

### **Fitting the Random Forest Regressor model**"""

model = RandomForestRegressor()
train(model, x_train, y_train)

"""
### **Fitting the Extra Trees Regressor Model**"""

model = ExtraTreesRegressor()
train(model, x_train, y_train)

"""
### **Fitting the LGBM Regressor Model**"""

model = LGBMRegressor()
train(model, x_train, y_train)

"""

### **Fitting the XGB Regressor Model**"""

model = XGBRegressor()
train(model, x_train, y_train)

"""

### **Fitting the Cat Boost Regressor Model**"""

model = CatBoostRegressor(verbose = 0)
train(model, x_train, y_train)

"""## **Randomized Search CV Algorithm for Random Forest Regressor**"""

random_grid = {
               'max_features': ['auto', 'sqrt'],
               'max_depth': [int(x) for x in np.linspace(5, 30, num = 6)],
               'min_samples_split':[2, 5, 10, 15, 100],
               'min_samples_leaf': [1, 2, 5, 10]
}

"""### **Display the Best Parameters, Best Score and R2 Score**"""

RF = RandomForestRegressor()
RF = RandomizedSearchCV(estimator = RF, param_distributions = random_grid, scoring = 'neg_mean_squared_error', n_iter =10,
                       verbose = 0, cv =10, random_state = 10, n_jobs = 1)
RF.fit(x_train, y_train)
print('Best Params : ',RF.best_params_,'\n')
print('Best Score : ',RF.best_score_,'\n')
prediction = RF.predict(x_test)
print('R2 Score : ',r2_score(y_test,prediction))

"""### **Visualizing the data distribution of the dependent test variable , predicted dependent variable of the Random Forest Regressor Model against the density distribution using Seaborn Distplot**"""

sns.distplot(y_test-prediction)
plt.show()

"""## **LGBM Regressor Model to run with Randomized Search CV Algorithm**"""

params = {
    "learning_rate": uniform(0.03, 0.3),
    "max_depth": randint(2, 6),
    "n_estimators": randint(100, 150),
    "subsample": uniform(0.6, 0.4)
}

"""### **Display the Best Parameters, Best Score and R2 Score**"""

lgb = LGBMRegressor()
lgb = RandomizedSearchCV(estimator = lgb, param_distributions = params, cv = 10, n_iter = 10, verbose = 0,
                        scoring = 'neg_mean_squared_error', n_jobs = 1, random_state = 10)
lgb.fit(x_train,y_train)
print('Best Params : ',lgb.best_params_,'\n')
print('Best Score : ',lgb.best_score_,'\n')
prediction = lgb.predict(x_test)
print('R2 Score : ',r2_score(y_test,prediction))

"""### **Visualizing the data distribution of the dependent test variable , predicted dependent variable of the LGBM Regressor Model against the density distribution using Seaborn Distplot**"""

sns.distplot(y_test-prediction)
plt.show()

"""## **XGB Regressor Model to run with Randomized Search CV Algorithm**"""

params = {
    "gamma": uniform(0, 0.5),
    "learning_rate": uniform(0.03, 0.3),
    "max_depth": randint(2, 6),
    "n_estimators": randint(100, 150),
    "subsample": uniform(0.6, 0.4)
}

"""### **Display the Best Parameters, Best Score and R2 Score**"""

xgb = XGBRegressor()
xgb = RandomizedSearchCV(estimator = xgb, param_distributions = params, cv = 10, n_iter = 10, verbose = 0,
                        scoring = 'neg_mean_squared_error', n_jobs = 1, random_state = 10)
xgb.fit(x_train, y_train)
print('Best Params : ',xgb.best_params_,'\n')
print('Best Score : ',xgb.best_score_,'\n')
prediction = xgb.predict(x_test)
print('R2 Score : ',r2_score(y_test,prediction))

"""### **Visualizing the data distribution of the dependent test variable , predicted dependent variable of the XGB Regressor Model against the density distribution using Seaborn Distplot**"""

sns.distplot(y_test-prediction)
plt.show()

"""## **CatBoost Regressor Model to run with Randomized Search CV Algorithm**"""

params = {
    "learning_rate": uniform(0.03, 0.3),
    "max_depth": randint(2, 6),
    "n_estimators": randint(100, 150),
    "subsample": uniform(0.6, 0.4)
}

"""### **Display the Best Parameters, Best Score and R2 Score**"""

cat = CatBoostRegressor(verbose = 0)
cat = RandomizedSearchCV(estimator = cat, param_distributions = params, cv = 10, n_iter = 10, verbose = 0,
                        scoring = 'neg_mean_squared_error', n_jobs = 1, random_state = 10)
cat.fit(x_train,y_train)
print('Best Params : ',cat.best_params_,'\n')
print('Best Score : ',cat.best_score_,'\n')
prediction = cat.predict(x_test)
print('R2 Score : ',r2_score(y_test,prediction))

"""### **Visualizing the data distribution of the dependent test variable , predicted dependent variable of the CatBoost Regressor Model against the density distribution using Seaborn Distplot**"""

sns.distplot(y_test-prediction)
plt.show()

"""## **Fitting the CatBoost Regressor Model**"""

cat = CatBoostRegressor(learning_rate = 0.08941885942788719, max_depth = 2, n_estimators = 109,
                        subsample = 0.6676443346250142, verbose = 0)
cat.fit(x_train,y_train)
predictions = cat.predict(x_test)
print('R2 score : ',r2_score(y_test,predictions))

"""# **6. Model Testing**"""

pickle.dump(cat,open('Model.pkl','wb'))

model = pickle.load(open('Model.pkl','rb'))
fpred = model.predict(x)
print('R2 Score of Full Data : ',r2_score(y,fpred))



"""# **Conclusion**

1. The code performs a comprehensive analysis of sales data, encompassing exploratory data analysis, data cleaning, visualization, and model training.

2.  Multiple regression models (Linear Regression, Ridge, Lasso, Decision Tree, Random Forest, Extra Trees, LGBM, XGBoost, and CatBoost) are trained and evaluated using metrics like R2 score and negative mean squared error.

3. Hyperparameter tuning is performed using RandomizedSearchCV for Random Forest, LGBM, XGBoost, and CatBoost regressors to optimize model performance.

4. Feature importance is visualized for tree-based models, providing insights into the most influential factors driving sales.

5. The best-performing model (CatBoost Regressor) is saved using pickle for future deployment and prediction on new data, demonstrating a complete machine learning workflow.

"""